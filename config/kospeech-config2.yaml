data:
    name: kospeech_5m_30ep
    vocab: C:\Users\USER\Desktop\jiwon\las-pytorch\data\kospeech\processed\idx2chap.csv
    batch_size: 16
    text: character
    train: C:\Users\USER\Desktop\jiwon\las-pytorch\data\kospeech\processed\train.csv
    test: C:\Users\USER\Desktop\jiwon\las-pytorch\data\kospeech\processed\test-clean.csv
    dev: C:\Users\USER\Desktop\jiwon\las-pytorch\data\kospeech\processed\dev.csv
    short_first: False
    num_mel_bins: 40
    num_works: 8
    vocab_size: 2000
model:
    listener:
        input_feature_dim: 40
        hidden_size: 512
        num_layers: 3
        dropout: 0.0
        bidirectional: True
        rnn_unit: "LSTM"
        use_gpu: True
    speller:
        hidden_size: 1024
        num_layers: 2
        bidirectional: True
        rnn_unit: "LSTM"
        vocab_size: 2000                           # 61 phonemes + 2 for <sos> & <eos>
        multi_head: 1                               # Number of heads for multi-head attention
        decode_mode: 1                              # Decoding mode, 0 : feed char distribution to next timestep, 1: feed argmax, 2: feed sampled vector
        use_mlp_in_attention: True                  # Set to False to exclude phi and psi in attention formula
        mlp_dim_in_attention: 64                   #
        mlp_activate_in_attention: 'relu'           #
        listener_hidden_size: 512
        max_label_len: 576
training:
    optimizer: 'adam'
    lr: 0.0002
    weight_decay: 0.000
    momentum: 0.0
    epochs: 30
    half_lr: 0.0
    early_stop: 0.0
    max_norm: 5
    save_folder: 'runs/'
    checkpoint: True
    continue_from: False
    tensorboard: True
    print_freq: 500
    label_smoothing: 0.1
    tf_rate_upperbound: 0.9                    # teacher forcing rate during training will be linearly
    tf_rate_lowerbound: 0.5                    # decaying from upperbound to lower bound for each epoch
    tf_decay_step: 100000
